{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["###Compression Methods\n","This notebook implements two compression techniques on the LSTM and ST-GCN models\n","1. Dynamic Quantisation\n","2. Knowlegde Distillation"],"metadata":{"id":"jn2SSAQgbdvu"}},{"cell_type":"code","source":["#Install libraries\n","!pip install torch_geometric"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6d5pdJqIPRjb","outputId":"e1dc617e-304c-41cc-ba59-2689a10460b7","executionInfo":{"status":"ok","timestamp":1747197547158,"user_tz":-60,"elapsed":16691,"user":{"displayName":"Sanjutha Indrajit","userId":"05378844996233805446"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torch_geometric\n","  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.11.15)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2025.3.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.1.6)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.0.2)\n","Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (5.9.5)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.2.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.32.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (4.67.1)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.6.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (6.4.3)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (0.3.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.20.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch_geometric) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2025.4.26)\n","Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: torch_geometric\n","Successfully installed torch_geometric-2.6.1\n"]}]},{"cell_type":"code","source":["#Import libraries\n","import tensorflow as tf\n","import time\n","import pandas as pd\n","import numpy as np\n","import os\n","import torch\n","import torch.nn as nn\n","import torch_geometric\n","from torch_geometric.data import Data, DataLoader\n","from sklearn.metrics import mean_absolute_error\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler\n","from sklearn.preprocessing import LabelEncoder\n","from torch_geometric.nn import GCNConv\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense, Dropout"],"metadata":{"id":"EY_2REgNb1FF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Can be uncommented if google colab is being used for running the script\n","# from google.colab import drive\n","# drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nnAu4uqthKF6","outputId":"0a77c5f0-0495-4672-ceb7-b7fede88a7b6","executionInfo":{"status":"ok","timestamp":1747197603290,"user_tz":-60,"elapsed":20194,"user":{"displayName":"Sanjutha Indrajit","userId":"05378844996233805446"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["#Change the model directory to the path the models were saved in modelling_lstm.ipynb and modelling_stgcn.ipynb\n","lstm_model_path = '/content/drive/MyDrive/AI_Sustainability/models/lstm_model_lag_14_4layer.h5'\n","stgcn_model_path = '/content/drive/MyDrive/AI_Sustainability/models/st_gcn_model_epoch_10.pth'\n","look_back = 14"],"metadata":{"id":"_9NDXwKscPQP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Load dataset\n","train_data = \"/content/drive/MyDrive/AI_Sustainability/data/train_data_ensemble_0-agg.csv\"\n","X = pd.read_csv(train_data)\n","train_dataset = X[['Date', 'min_dist_node', 'stid', 'tcolc_eatm_0', 'ulwrf_tatm_0', 'dlwrf_sfc_0', 'tmp_sfc_0', 'tcdc_eatm_0', 'dswrf_sfc_0',\n","            'tmax_2m_0', 'tmin_2m_0', 'pwat_eatm_0', 'ulwrf_tatm_0', 'dlwrf_sfc_0', 'tmp_sfc_0',\n","            'uswrf_sfc_0', 'spfh_2m_0', 'ulwrf_sfc_0', 'tmp_2m_0', 'apcp_sfc_0', 'pres_msl_0', 'Daily_Production']]\n","\n","test_data = \"/content/drive/MyDrive/AI_Sustainability/data/test_data_ensemble_0-agg.csv\"\n","X = pd.read_csv(test_data)\n","test_dataset = X[['Date', 'min_dist_node','stid', 'tcolc_eatm_0', 'ulwrf_tatm_0', 'dlwrf_sfc_0', 'tmp_sfc_0', 'tcdc_eatm_0', 'dswrf_sfc_0',\n","            'tmax_2m_0', 'tmin_2m_0', 'pwat_eatm_0', 'ulwrf_tatm_0', 'dlwrf_sfc_0', 'tmp_sfc_0',\n","            'uswrf_sfc_0', 'spfh_2m_0', 'ulwrf_sfc_0', 'tmp_2m_0', 'apcp_sfc_0', 'pres_msl_0', 'Daily_Production']]\n"],"metadata":{"id":"Ez-7N56NVEF9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Data Preprocessing"],"metadata":{"id":"Ycnzy7ntcQEg"}},{"cell_type":"code","source":["def preprocess_data(df):\n","    \"\"\"\n","    Data preprocessing that adds columns ['day', 'month', 'year'], extract latitude and longitude from 'min_dist_node' column and interpolates values.\n","    Station ID is also encoded using one-hot encoding.\n","    \"\"\"\n","    # Convert date to datetime\n","    df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n","\n","    # Sort by station and date\n","    df = df.sort_values([\"stid\", \"Date\"]).reset_index(drop=True)\n","\n","    # Extract weather features\n","    weather_features = [col for col in df.columns if col.endswith(\"_0\")]\n","\n","    df[\"dayofyear\"] = df[\"Date\"].dt.dayofyear\n","    df[\"month\"] = df[\"Date\"].dt.month\n","    df[\"weekday\"] = df[\"Date\"].dt.weekday\n","\n","    df[\"min_dist_node\"] = df[\"min_dist_node\"].astype(str)\n","\n","    def extract_float_tuple(s):\n","        try:\n","            return tuple(float(part.replace(\"np.float32(\", \"\").replace(\")\", \"\")) for part in s.strip(\"()\").split(\",\"))\n","        except:\n","            return (None, None)\n","\n","    df[['lat', 'lon']] = df['min_dist_node'].apply(lambda x: pd.Series(extract_float_tuple(str(x))))\n","\n","    df[\"lat\"] = df[\"lat\"].fillna(method=\"ffill\")\n","    df[\"lon\"] = df[\"lon\"].fillna(method=\"ffill\")\n","\n","    le = LabelEncoder()\n","    df[\"stid_encoded\"] = le.fit_transform(df[\"stid\"])\n","\n","    return df, le, weather_features"],"metadata":{"id":"BMmNdpplVvsu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_df, station_encoder, weather_features = preprocess_data(train_dataset)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3kli6KdkVwwo","outputId":"58245ba5-2ad0-443b-cb55-23b2fa18ad8c","executionInfo":{"status":"ok","timestamp":1747197649485,"user_tz":-60,"elapsed":22232,"user":{"displayName":"Sanjutha Indrajit","userId":"05378844996233805446"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-6-d200071b887c>:7: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n","<ipython-input-6-d200071b887c>:29: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n","  df[\"lat\"] = df[\"lat\"].fillna(method=\"ffill\")\n","<ipython-input-6-d200071b887c>:30: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n","  df[\"lon\"] = df[\"lon\"].fillna(method=\"ffill\")\n"]}]},{"cell_type":"code","source":["test_df, _, _ = preprocess_data(test_dataset)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KUZfwYtHTc7U","outputId":"d5c1135e-4d77-4027-9c0b-6130305faeaf","executionInfo":{"status":"ok","timestamp":1747197653247,"user_tz":-60,"elapsed":3766,"user":{"displayName":"Sanjutha Indrajit","userId":"05378844996233805446"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-6-d200071b887c>:7: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n","<ipython-input-6-d200071b887c>:29: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n","  df[\"lat\"] = df[\"lat\"].fillna(method=\"ffill\")\n","<ipython-input-6-d200071b887c>:30: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n","  df[\"lon\"] = df[\"lon\"].fillna(method=\"ffill\")\n"]}]},{"cell_type":"markdown","source":["##Temporal Sequence Creation"],"metadata":{"id":"EmikBmqzce_t"}},{"cell_type":"code","source":["def create_sequences(data, stations, features, target_col, lookback=7):\n","    \"\"\"\n","    Function to created shifted window sequences for all stations for a given look back window\n","    \"\"\"\n","    X_sequences = []\n","    y_values = []\n","    station_ids = []\n","    dates = []\n","\n","    # Additional features to include (non-weather)\n","    additional_features = ['dayofyear', 'month', 'weekday', 'lat', 'lon', 'stid_encoded']\n","    all_features = features + additional_features\n","\n","    for station in stations:\n","        # Get data for this station\n","        station_data = data[data['stid'] == station].copy()\n","        if len(station_data) <= lookback:\n","            continue\n","\n","        # Create sequences for this station\n","        station_features = station_data[all_features].values\n","        station_target = station_data[target_col].values\n","        station_dates = station_data['Date'].values\n","\n","        for i in range(len(station_data) - lookback):\n","            X_sequences.append(station_features[i:i+lookback])\n","            y_values.append(station_target[i+lookback])\n","            station_ids.append(station)\n","            dates.append(station_dates[i+lookback])\n","\n","    return np.array(X_sequences), np.array(y_values), np.array(station_ids), np.array(dates)"],"metadata":{"id":"o4Yeu-L_WKE1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Extract features and targets\n","features_cols = weather_features\n","target_col = 'Daily_Production'\n","station_list = train_df['stid'].unique()\n","\n","# Create sequences for training data\n","X_train_seq, y_train, train_stations, train_dates = create_sequences(\n","    train_df, station_list, features_cols, target_col, lookback=look_back\n",")\n","\n","# Create sequences for test data\n","X_test_seq, y_test, test_stations, test_dates = create_sequences(\n","    test_df, station_list, features_cols, target_col, lookback=look_back\n",")\n","\n","print(f\"Training sequences: {X_train_seq.shape}\")\n","print(f\"Training targets: {y_train.shape}\")\n","print(f\"Test sequences: {X_test_seq.shape}\")\n","print(f\"Test targets: {y_test.shape}\")\n","\n","X_val_seq, y_val = X_train_seq[-1000:], y_train[-1000:]\n","X_train_seq, y_train = X_train_seq[:-1000], y_train[:-1000]"],"metadata":{"id":"uLbqhJCOWK-7","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a536d130-b462-4962-ba5d-b5bcb47e2e3b","executionInfo":{"status":"ok","timestamp":1747197656622,"user_tz":-60,"elapsed":3368,"user":{"displayName":"Sanjutha Indrajit","userId":"05378844996233805446"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training sequences: (177674, 14, 30)\n","Training targets: (177674,)\n","Test sequences: (34398, 14, 30)\n","Test targets: (34398,)\n"]}]},{"cell_type":"markdown","source":["##Feature Scaling"],"metadata":{"id":"rXJMInnhclnc"}},{"cell_type":"code","source":["# Normalize features\n","scaler_X = StandardScaler()\n","# Reshape to 2D for scaling\n","n_samples_train, n_timesteps, n_features = X_train_seq.shape\n","X_train_reshaped = X_train_seq.reshape(n_samples_train * n_timesteps, n_features)\n","X_train_scaled = scaler_X.fit_transform(X_train_reshaped)\n","# Reshape back to 3D\n","X_train_scaled = X_train_scaled.reshape(n_samples_train, n_timesteps, n_features)\n","\n","# Scale test data using the same scaler\n","n_samples_test, _, _ = X_test_seq.shape\n","X_test_reshaped = X_test_seq.reshape(n_samples_test * n_timesteps, n_features)\n","X_test_scaled = scaler_X.transform(X_test_reshaped)\n","X_test_scaled = X_test_scaled.reshape(n_samples_test, n_timesteps, n_features)\n","\n","# Scale target\n","scaler_y = MinMaxScaler()\n","y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()\n","y_test_scaled = scaler_y.transform(y_test.reshape(-1, 1)).flatten()\n","\n","X_val_scaled, y_val_scaled = X_train_scaled[-1000:], y_train_scaled[-1000:]\n","X_train_scaled, y_train_scaled = X_train_scaled[:-1000], y_train_scaled[:-1000]"],"metadata":{"id":"ZUaO1HWSSEPY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def model_size(filepath):\n","  \"\"\"\n","  Function to calculate model size in KB\n","  \"\"\"\n","  if not os.path.exists(filepath):\n","      raise FileNotFoundError(f\"No such file: {filepath}\")\n","  size_kb = os.path.getsize(filepath) / 1024\n","  print(f\"File size of '{filepath}': {size_kb:.2f} KB\")\n","  return size_kb"],"metadata":{"id":"6KFmsZ8rblJJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Post Training Quantisation\n","Here since two different frameworks were used to create the LSTM and STGCN models (Tensorflow Keras and Pytorch respectively), their corresponding post training quantistaion modules are used for compression"],"metadata":{"id":"_2Dh8kNaSEIv"}},{"cell_type":"code","source":["def post_training_quantization_tf(model_path, save_path, model='lstm'):\n","    \"\"\"\n","    Function to perform post-training quantisation\n","    \"\"\"\n","    if model == \"lstm\":\n","      # Load model\n","      model = tf.keras.models.load_model(model_path, custom_objects={'mse': tf.keras.losses.MeanSquaredError})\n","\n","      # Convert to a quantized TFLite model\n","      converter = tf.lite.TFLiteConverter.from_keras_model(model)\n","      converter.optimizations = [tf.lite.Optimize.DEFAULT]  # Default setting with float32 to int8 quantisation\n","      # converter.target_spec.supported_types = [tf.float16]\n","      converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n","      converter._experimental_lower_tensor_list_ops = False\n","      tflite_model = converter.convert()\n","      save_dir = os.path.dirname(save_path)\n","\n","      # Save quantized model\n","      with open(save_path, 'wb') as f:\n","          f.write(tflite_model)\n","\n","      print(f\"Quantized model saved to: {save_path}\")\n","      return save_path\n","    else:\n","      class STGCNModel(nn.Module):\n","        def __init__(self, num_node_features, hidden_dim):\n","            super(STGCNModel, self).__init__()\n","            self.gcn1 = GCNConv(num_node_features, hidden_dim)\n","            self.gcn2 = GCNConv(hidden_dim, hidden_dim)\n","            self.fc = nn.Linear(hidden_dim, 1)\n","\n","        def forward(self, x, edge_index, batch):\n","            x = torch.relu(self.gcn1(x, edge_index))\n","            x = torch.relu(self.gcn2(x, edge_index))\n","            # Apply global pooling to get a graph-level representation\n","            x = torch_geometric.nn.global_mean_pool(x, batch)\n","            x = self.fc(x)\n","            return x.squeeze()\n","      model = STGCNModel(num_node_features=30, hidden_dim=64)\n","      model.load_state_dict(torch.load(model_path, map_location=\"cpu\"))\n","      model.eval()\n","      model.qconfig = torch.quantization.get_default_qconfig(\"fbgemm\")\n","      model_prepared = torch.quantization.prepare(model)\n","      torch.save(model_prepared.state_dict(), save_path)\n","      print(f\"Quantized model saved to: {save_path}\")\n","      return model_prepared\n"],"metadata":{"id":"Qck-fY_nYT3g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["LSTM"],"metadata":{"id":"CZ4aMv5HYwSg"}},{"cell_type":"code","source":["quant_lstm_model = post_training_quantization_tf(lstm_model_path, '/lstm_quantized_model.tflite', 'lstm')\n","print(f\"LSTM Model size after quantisation: {model_size('/lstm_quantized_model.tflite')} KB\")\n","print(f\"LSTM Model size before quantisation: {model_size(lstm_model_path)} KB\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jwvXKChJYrnZ","outputId":"0d545ece-f5b9-4100-ffc3-26e943465b7f","executionInfo":{"status":"ok","timestamp":1747197834690,"user_tz":-60,"elapsed":2006,"user":{"displayName":"Sanjutha Indrajit","userId":"05378844996233805446"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"]},{"output_type":"stream","name":"stdout","text":["Saved artifact at '/tmp/tmp7zv87yji'. The following endpoints are available:\n","\n","* Endpoint 'serve'\n","  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 14, 30), dtype=tf.float32, name='input_layer')\n","Output Type:\n","  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n","Captures:\n","  134257162913168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  134257162925648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  134257162912400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  134256709714192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  134256709720144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  134256709719760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  134256709719568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  134256709721104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  134256709718800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  134256709715536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  134256709718608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","Quantized TFLite model saved to: /lstm_quantized_model.tflite\n","File size of '/lstm_quantized_model.tflite': 190.01 KB\n","LSTM Model size after quantisation: 190.0078125 KB\n","File size of '/content/drive/MyDrive/ams-2014-solar-energy-prediction/models/lstm_model_lag_14_4layer.h5': 1722.70 KB\n","LSTM Model size before quantisation: 1722.703125 KB\n"]}]},{"cell_type":"code","source":["# Run inference on tflite model\n","interpreter = tf.lite.Interpreter(model_path='/lstm_quantized_model.tflite')\n","interpreter.allocate_tensors()\n","\n","input_details = interpreter.get_input_details()\n","output_details = interpreter.get_output_details()\n","\n","input_data = X_test_scaled.astype(input_details[0]['dtype'])\n","\n","preds = []\n","start = time.time()\n","for i in range(input_data.shape[0]):\n","    interpreter.set_tensor(input_details[0]['index'], input_data[i:i+1])\n","    interpreter.invoke()\n","    output_data = interpreter.get_tensor(output_details[0]['index'])\n","    preds.append(output_data[0])\n","end = time.time()\n","print(f\"Time taken for inference: {end - start} seconds\")\n","preds = np.array(preds)"],"metadata":{"id":"ha9FXr7IGbWt","executionInfo":{"status":"ok","timestamp":1747199492977,"user_tz":-60,"elapsed":37899,"user":{"displayName":"Sanjutha Indrajit","userId":"05378844996233805446"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"f693f4ac-f137-4cb1-c68f-c93b6486fd18"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Time taken for inference: 37.71252655982971 seconds\n"]}]},{"cell_type":"code","source":["# Inverse transform the predictions and true values\n","y_pred = scaler_y.inverse_transform(preds.reshape(-1, 1))\n","y_true = scaler_y.inverse_transform(y_test_scaled.reshape(-1, 1))\n","\n","# Calculate Mean Absolute Error\n","mae = mean_absolute_error(y_true, y_pred)\n","print(f\"Mean Absolute Error (MAE) after Quantisation of LSTM Model: {mae}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WX2qKnRRHRH0","executionInfo":{"status":"ok","timestamp":1747198090611,"user_tz":-60,"elapsed":33,"user":{"displayName":"Sanjutha Indrajit","userId":"05378844996233805446"}},"outputId":"94e5422b-474a-4f99-af45-b07334594dc1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mean Absolute Error (MAE) after Quantisation of LSTM Model: 4456819.899176551\n"]}]},{"cell_type":"markdown","source":["ST-GCN"],"metadata":{"id":"ceqNXpspSexa"}},{"cell_type":"code","source":["stgcn_quant_model = post_training_quantization_tf(stgcn_model_path, '/stgcn_quantized_model.pth', 'GCN')\n","print(f\"STGCN Model size after quantisation: {model_size('/stgcn_quantized_model.pth')} KB\")\n","print(f\"STGCN Model size before quantisation: {model_size(stgcn_model_path)} KB\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7LpSgPYeRTLY","outputId":"c21af358-3b58-4f6b-95a3-5a215a12ca79","executionInfo":{"status":"ok","timestamp":1747198511902,"user_tz":-60,"elapsed":49,"user":{"displayName":"Sanjutha Indrajit","userId":"05378844996233805446"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Quantized PyTorch model saved to: /stgcn_quantized_model.pth\n","File size of '/stgcn_quantized_model.pth': 36.18 KB\n","STGCN Model size after quantisation: 36.18359375 KB\n","File size of '/content/drive/MyDrive/ams-2014-solar-energy-prediction/models/st_gcn_model_epoch_1.pth': 27.07 KB\n","STGCN Model size before quantisation: 27.072265625 KB\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torch/ao/quantization/observer.py:229: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["class STGCNModel(nn.Module):\n","    def __init__(self, num_node_features, hidden_dim):\n","        super(STGCNModel, self).__init__()\n","        self.gcn1 = GCNConv(num_node_features, hidden_dim)\n","        self.gcn2 = GCNConv(hidden_dim, hidden_dim)\n","        self.fc = nn.Linear(hidden_dim, 1)\n","\n","    def forward(self, x, edge_index, batch):\n","        x = torch.relu(self.gcn1(x, edge_index))\n","        x = torch.relu(self.gcn2(x, edge_index))\n","        # Apply global pooling to get a graph-level representation\n","        x = torch_geometric.nn.global_mean_pool(x, batch)\n","        x = self.fc(x)\n","        return x.squeeze()\n","\n","# Re-initialize the model architecture\n","stgcn_quant_model.load_state_dict(torch.load('/stgcn_quantized_model.pth', map_location=\"cpu\"))\n","stgcn_quant_model.eval()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i3y4-6y7HWjx","executionInfo":{"status":"ok","timestamp":1747198683276,"user_tz":-60,"elapsed":13,"user":{"displayName":"Sanjutha Indrajit","userId":"05378844996233805446"}},"outputId":"007386b0-1d11-4fbc-eb01-ef5c7ecb729e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["STGCNModel(\n","  (gcn1): GCNConv(30, 64)\n","  (gcn2): GCNConv(64, 64)\n","  (fc): Linear(\n","    in_features=64, out_features=1, bias=True\n","    (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n","  )\n",")"]},"metadata":{},"execution_count":45}]},{"cell_type":"code","source":["preds = []\n","X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n","\n","preds = []\n","\n","start = time.time()\n","for i in range(X_test_scaled.shape[0]):\n","    sample_sequence = X_test_scaled[i]\n","\n","    x = torch.tensor(sample_sequence, dtype=torch.float32)\n","\n","    x = x.view(look_back, -1)\n","    edge_index = torch.empty((2, 0), dtype=torch.long)\n","    batch = torch.zeros(look_back, dtype=torch.long)\n","\n","    # Perform inference\n","    with torch.no_grad():\n","        prediction = stgcn_quant_model(x, edge_index, batch)\n","\n","    preds.append(prediction.item())\n","end = time.time()\n","print(f\"Time taken for inference: {end - start} seconds\")\n","\n","preds = np.array(preds) # Convert list of predictions to a NumPy array\n","\n","y_pred = scaler_y.inverse_transform(preds.reshape(-1, 1))\n","y_true = scaler_y.inverse_transform(y_test_scaled.reshape(-1, 1))\n","\n","# Calculate Mean Absolute Error\n","mae = mean_absolute_error(y_true, y_pred)\n","print(f\"Mean Absolute Error (MAE) for STGCN Model: {mae}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uOmnJgkZIfV6","executionInfo":{"status":"ok","timestamp":1747199898464,"user_tz":-60,"elapsed":46774,"user":{"displayName":"Sanjutha Indrajit","userId":"05378844996233805446"}},"outputId":"a78a66a3-7abb-44a9-ca6c-1cc9e8ce1d15"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Time taken for inference: 46.592411279678345 seconds\n","Mean Absolute Error (MAE) for STGCN Model: 4340312.914298176\n"]}]},{"cell_type":"markdown","source":["##Distillation"],"metadata":{"id":"7MWaTC3xSjWN"}},{"cell_type":"markdown","source":["##LSTM Distillation"],"metadata":{"id":"Zm3OEfJ4bojB"}},{"cell_type":"markdown","source":["##LSTM Model Definiton"],"metadata":{"id":"gI1Sk-5Kc5ih"}},{"cell_type":"code","source":["# Import libraries needed for distillation\n","import tensorflow as tf\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import time\n","import numpy as np\n","from tensorflow.keras.layers import LSTM, Dense\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.callbacks import EarlyStopping\n","from torch_geometric.nn import GCNConv\n","from sklearn.metrics import mean_absolute_error\n","\n","\"\"\"## LSTM Distillation\n","Create a smaller student LSTM model and train it to mimic our larger teacher LSTM model\n","\"\"\"\n","\n","def create_student_lstm_model(input_shape, hidden_units=64):\n","    \"\"\"\n","    Creates a smaller LSTM model with fewer parameters than the teacher\n","    \"\"\"\n","    model = Sequential()\n","    model.add(LSTM(units=hidden_units, input_shape=input_shape))\n","    model.add(Dense(1))\n","    model.compile(optimizer='adam', loss='mse')\n","    return model\n","\n","# Define distillation loss function for Tensorflow LSTM\n","def distillation_loss(alpha=0.5):\n","    \"\"\"\n","    Create a loss function that combines:\n","    - standard MSE against true values\n","    - distillation loss (MSE between student & teacher predictions)\n","\n","    Args:\n","        alpha: weight for balancing the two losses (0-1)\n","    \"\"\"\n","    mse = tf.keras.losses.MeanSquaredError()\n","    def loss_fn(y_true, y_pred):\n","        # Extract the true target and teacher predictions\n","        # y_true is expected to have shape (batch_size, 2) where:\n","        # - [:, 0] contains the actual targets\n","        # - [:, 1] contains the teacher predictions\n","        true_targets = y_true[:, 0]\n","        teacher_preds = y_true[:, 1]\n","\n","        # Standard MSE loss against true targets\n","        mse_loss = mse(true_targets, y_pred)\n","\n","        # Distillation loss (MSE between student & teacher predictions)\n","        distill_loss = mse(teacher_preds, y_pred)\n","\n","        # Combined loss\n","        return alpha * mse_loss + (1 - alpha) * distill_loss\n","\n","    return loss_fn\n","\n","# Load the teacher LSTM model\n","custom_objects_for_loading = {'mse': tf.keras.losses.MeanSquaredError()}\n","lstm_teacher_model = tf.keras.models.load_model(lstm_model_path, custom_objects=custom_objects_for_loading)\n","\n","# Generate teacher predictions for training data\n","teacher_train_preds = lstm_teacher_model.predict(X_train_scaled)\n","teacher_val_preds = lstm_teacher_model.predict(X_val_scaled)\n","\n","# Prepare the training data for distillation\n","# Combine true targets with teacher predictions\n","y_train_combined = np.column_stack([y_train_scaled, teacher_train_preds.flatten()])\n","y_val_combined = np.column_stack([y_val_scaled, teacher_val_preds.flatten()])\n","\n","# Create student model\n","input_shape = (look_back, X_train_scaled.shape[2])\n","lstm_student_model = create_student_lstm_model(input_shape)\n","\n","print(\"Teacher LSTM Model:\")\n","lstm_teacher_model.summary()\n","\n","print(\"\\nStudent LSTM Model:\")\n","lstm_student_model.summary()\n","\n","# Train the student model with distillation\n","early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n","\n","# Custom loss function\n","student_loss = distillation_loss(alpha=0.3)\n","\n","# Compile with custom loss\n","lstm_student_model.compile(optimizer='adam', loss=student_loss)\n","\n","train_start = time.time()\n","history = lstm_student_model.fit(\n","    X_train_scaled,\n","    y_train_combined,\n","    epochs=5,\n","    batch_size=128,\n","    validation_data=(X_val_scaled, y_val_combined),\n","    callbacks=[early_stopping],\n","    verbose=1\n",")\n","train_end = time.time()\n","print(f\"Training time for student LSTM model: {train_end - train_start:.2f} seconds\")\n","\n","# Save the student model\n","lstm_student_model.save('/lstm_student_model.h5')\n","\n","# Evaluate student model\n","student_preds = lstm_student_model.predict(X_test_scaled)\n","y_student_pred = scaler_y.inverse_transform(student_preds.reshape(-1, 1))\n","student_mae = mean_absolute_error(y_test, y_student_pred)\n","print(f\"Student LSTM Model MAE: {student_mae:.4f}\")\n","\n","# Size comparison\n","print(f\"Teacher LSTM Model size: {model_size(lstm_model_path)} KB\")\n","print(f\"Student LSTM Model size: {model_size('/lstm_student_model.h5')} KB\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":942},"id":"jRO6YYr9ZpF-","executionInfo":{"status":"ok","timestamp":1747203950907,"user_tz":-60,"elapsed":264324,"user":{"displayName":"Sanjutha Indrajit","userId":"05378844996233805446"}},"outputId":"aee38fd6-a3f4-490a-a7c1-4d4617b3ec3f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m5490/5490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 13ms/step\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n","Teacher LSTM Model:\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(**kwargs)\n"]},{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"sequential\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │        \u001b[38;5;34m81,408\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m49,408\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │        \u001b[38;5;34m12,416\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">81,408</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,416</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m143,267\u001b[0m (559.64 KB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">143,267</span> (559.64 KB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m143,265\u001b[0m (559.63 KB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">143,265</span> (559.63 KB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2\u001b[0m (12.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> (12.00 B)\n","</pre>\n"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Student LSTM Model:\n"]},{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"sequential_4\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ lstm_8 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m24,320\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ lstm_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">24,320</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m24,385\u001b[0m (95.25 KB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">24,385</span> (95.25 KB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m24,385\u001b[0m (95.25 KB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">24,385</span> (95.25 KB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","\u001b[1m1373/1373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 25ms/step - loss: 0.0350 - val_loss: 0.0167\n","Epoch 2/5\n","\u001b[1m1373/1373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 21ms/step - loss: 0.0174 - val_loss: 0.0126\n","Epoch 3/5\n","\u001b[1m1373/1373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 21ms/step - loss: 0.0130 - val_loss: 0.0090\n","Epoch 4/5\n","\u001b[1m1373/1373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 23ms/step - loss: 0.0099 - val_loss: 0.0073\n","Epoch 5/5\n","\u001b[1m1373/1373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 21ms/step - loss: 0.0080 - val_loss: 0.0058\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training time for student LSTM model: 176.24 seconds\n","\u001b[1m1075/1075\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step\n","Student LSTM Model MAE: 5025533.0000\n","File size of '/content/drive/MyDrive/ams-2014-solar-energy-prediction/models/lstm_model_lag_14_4layer.h5': 1722.70 KB\n","Teacher LSTM Model size: 1722.703125 KB\n","File size of '/lstm_student_model.h5': 309.17 KB\n","Student LSTM Model size: 309.171875 KB\n"]}]},{"cell_type":"markdown","source":["##Graph Creation"],"metadata":{"id":"iv25FD38cv4R"}},{"cell_type":"code","source":["def create_graph_data(X_seq, y_seq):\n","    data_list = []\n","    for i in range(len(X_seq)):\n","        x = torch.tensor(X_seq[i], dtype=torch.float)  # shape (nodes, features)\n","        num_nodes = x.size(0)\n","        edge_index = torch.combinations(torch.arange(num_nodes), r=2).T\n","        edge_index = torch.cat([edge_index, edge_index[[1, 0]]], dim=1)  # undirected\n","        data = Data(x=x, edge_index=edge_index, y=torch.tensor([y_seq[i]], dtype=torch.float))\n","        data_list.append(data)\n","    return data_list\n","train_graphs = create_graph_data(X_train_scaled, y_train_scaled)"],"metadata":{"id":"lKGCiCzBfA5Q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# STGCN Model Distillation"],"metadata":{"id":"AG9zZryHdBY4"}},{"cell_type":"markdown","source":["##STGCN Model Definition"],"metadata":{"id":"KqqkNHvec0Qt"}},{"cell_type":"code","source":["\n","class STGCNStudentModel(nn.Module):\n","    \"\"\"\n","    A smaller version of the STGCN model with fewer parameters\n","    \"\"\"\n","    def __init__(self, num_node_features, hidden_dim=32):\n","        super(STGCNStudentModel, self).__init__()\n","        self.gcn = GCNConv(num_node_features, hidden_dim)\n","        self.fc = nn.Linear(hidden_dim, 1)\n","\n","    def forward(self, x, edge_index, batch):\n","        x = torch.relu(self.gcn(x, edge_index))\n","        x = torch_geometric.nn.global_mean_pool(x, batch)\n","        x = self.fc(x)\n","        return x.squeeze()\n","\n","# Load teacher model\n","class STGCNModel(nn.Module):\n","    def __init__(self, num_node_features, hidden_dim):\n","        super(STGCNModel, self).__init__()\n","        self.gcn1 = GCNConv(num_node_features, hidden_dim)\n","        self.gcn2 = GCNConv(hidden_dim, hidden_dim)\n","        self.fc = nn.Linear(hidden_dim, 1)\n","\n","    def forward(self, x, edge_index, batch):\n","        x = torch.relu(self.gcn1(x, edge_index))\n","        x = torch.relu(self.gcn2(x, edge_index))\n","        x = torch_geometric.nn.global_mean_pool(x, batch)\n","        x = self.fc(x)\n","        return x.squeeze()\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Initialize teacher model\n","stgcn_teacher_model = STGCNModel(num_node_features=X_train_scaled.shape[2], hidden_dim=64).to(device)\n","stgcn_teacher_model.load_state_dict(torch.load(stgcn_model_path, map_location=device))\n","stgcn_teacher_model.eval()\n","\n","# Initialize student model\n","stgcn_student_model = STGCNStudentModel(num_node_features=X_train_scaled.shape[2], hidden_dim=32).to(device)\n","\n","# Count parameters\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(f\"Teacher STGCN model parameters: {count_parameters(stgcn_teacher_model)}\")\n","print(f\"Student STGCN model parameters: {count_parameters(stgcn_student_model)}\")\n","\n","# Create a new train loader with a smaller batch size for distillation\n","train_loader_distill = DataLoader(train_graphs, batch_size=128, shuffle=True)\n","\n","# Optimizer\n","optimizer = torch.optim.Adam(stgcn_student_model.parameters(), lr=0.001)\n","\n","# Loss functions\n","mse_loss = nn.MSELoss()\n","distill_loss = nn.MSELoss()\n","\n","# Train the student model\n","stgcn_student_model.train()\n","alpha = 0.3  # Weight for balancing between true loss and distillation loss\n","\n","train_start = time.time()\n","for epoch in range(10):\n","    total_loss = 0\n","    for batch in train_loader_distill:\n","        batch = batch.to(device)\n","\n","        # Forward pass student model\n","        student_out = stgcn_student_model(batch.x, batch.edge_index, batch.batch)\n","\n","        # Get teacher predictions\n","        with torch.no_grad():\n","            teacher_out = stgcn_teacher_model(batch.x, batch.edge_index, batch.batch)\n","\n","        # Calculate losses\n","        loss_true = mse_loss(student_out, batch.y)  # Loss against true values\n","        loss_distill = distill_loss(student_out, teacher_out)  # Loss against teacher predictions\n","\n","        # Combined loss\n","        loss = alpha * loss_true + (1 - alpha) * loss_distill\n","\n","        # Backward and optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","\n","    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n","\n","train_end = time.time()\n","print(f\"Training time for student STGCN model: {train_end - train_start:.2f} seconds\")\n","\n","# Save student model\n","torch.save(stgcn_student_model.state_dict(), '/stgcn_student_model.pth')\n","\n","# Evaluate student model\n","stgcn_student_model.eval()\n","preds = []\n","\n","with torch.no_grad():\n","    for i in range(X_test_scaled.shape[0]):\n","        sample_sequence = X_test_scaled[i]\n","        x = torch.tensor(sample_sequence, dtype=torch.float32).to(device)\n","        x = x.view(look_back, -1)\n","\n","        edge_index = torch.empty((2, 0), dtype=torch.long).to(device)\n","        batch = torch.zeros(look_back, dtype=torch.long).to(device)\n","\n","        prediction = stgcn_student_model(x, edge_index, batch)\n","        preds.append(prediction.item())\n","\n","preds = np.array(preds)\n","y_student_pred = scaler_y.inverse_transform(preds.reshape(-1, 1))\n","student_mae = mean_absolute_error(y_test, y_student_pred)\n","print(f\"Student STGCN Model MAE: {student_mae:.4f}\")\n","\n","# Size comparison\n","print(f\"Teacher STGCN Model size: {model_size(stgcn_model_path)} KB\")\n","print(f\"Student STGCN Model size: {model_size('/stgcn_student_model.pth')} KB\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YjqGP8Qaci8m","executionInfo":{"status":"ok","timestamp":1747204721624,"user_tz":-60,"elapsed":435680,"user":{"displayName":"Sanjutha Indrajit","userId":"05378844996233805446"}},"outputId":"a39cd2ea-dd39-4695-fcc4-b9d07cd68f45"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Teacher STGCN model parameters: 6209\n","Student STGCN model parameters: 1025\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n","  warnings.warn(out)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Loss: 18.3406\n","Epoch 2, Loss: 12.8995\n","Epoch 3, Loss: 12.7256\n","Epoch 4, Loss: 12.6443\n","Epoch 5, Loss: 12.5817\n","Epoch 6, Loss: 12.5418\n","Epoch 7, Loss: 12.5140\n","Epoch 8, Loss: 12.4878\n","Epoch 9, Loss: 12.4610\n","Epoch 10, Loss: 12.4522\n","Training time for student STGCN model: 412.71 seconds\n","Student STGCN Model MAE: 3974047.5527\n","File size of '/content/drive/MyDrive/ams-2014-solar-energy-prediction/models/st_gcn_model_epoch_1.pth': 27.07 KB\n","Teacher STGCN Model size: 27.072265625 KB\n","File size of '/stgcn_student_model.pth': 6.17 KB\n","Student STGCN Model size: 6.171875 KB\n"]}]}]}